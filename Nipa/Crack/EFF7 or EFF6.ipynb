{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import skimage.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#사이킷런\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold #KFOLD\n",
    "#eval metric\n",
    "# from sklearn.metrics import recall_score \n",
    "# from sklearn.metrics import precision_score \n",
    "# from sklearn.metrics import f1_score \n",
    "\n",
    "#keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "#efn model\n",
    "import efficientnet.tfkeras as efn\n",
    "\n",
    "#etc\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow_addons.optimizers import AdamW\n",
    "seed = 7777\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def recall(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
    "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
    "\n",
    "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
    "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
    "\n",
    "    # Precision = (True Positive) / (True Positive + False Positive)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1score(y_target, y_pred):\n",
    "    _recall = recall(y_target, y_pred)\n",
    "    _precision = precision(y_target, y_pred)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
    "    \n",
    "    # return a single tensor value\n",
    "    return _f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_dir = '../../data/.train/.task153/data/train/'\n",
    "df = pd.read_csv(task_dir+'train.csv',names=['file','tile','label'],sep=',',skiprows=1)\n",
    "#image_file_list = glob(task_dir+ '/**/*.tif', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(image_path, resize_ratio=2):\n",
    "    if not(isinstance(image_path, str)):\n",
    "        # if tensor with byte string\n",
    "        image_path = image_path.numpy().decode('utf-8')\n",
    "\n",
    "    image_level_1 = skimage.io.MultiImage(image_path)[0]\n",
    "    if resize_ratio != 1:\n",
    "        new_w = int(image_level_1.shape[1]*resize_ratio)\n",
    "        new_h = int(image_level_1.shape[0]*resize_ratio)\n",
    "        image_level_1 = cv2.resize(\n",
    "            image_level_1, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    return image_level_1\n",
    "\n",
    "\n",
    "image = read_image(task_dir + '23.tif')\n",
    "print(image.shape)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lrfn(lr_start=0.00001, lr_max=0.00005,lr_min=0.00001, lr_rampup_epochs=5,lr_sustain_epochs=0, lr_exp_decay=.8):\n",
    "    \n",
    "#     lr_max = lr_max * strategy.num_replicas_in_sync\n",
    "    lr_max = lr_max * 8\n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_rampup_epochs:\n",
    "            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n",
    "        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n",
    "            lr = lr_max\n",
    "        else:\n",
    "            lr = (lr_max - lr_min) *\\\n",
    "                 lr_exp_decay**(epoch - lr_rampup_epochs- lr_sustain_epochs) + lr_min\n",
    "        return lr\n",
    "    return lrfn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "def Eff_B7():\n",
    "    model = Sequential([efn.EfficientNetB7(input_shape=(136,136,3),weights='noisy-student',include_top=False),\n",
    "                                 tf.keras.layers.GlobalAveragePooling2D(),\n",
    "                                 tf.keras.layers.BatchNormalization(),\n",
    "                                 tf.keras.layers.Dense(128,activation='relu',kernel_initializer='he_normal'),\n",
    "                                 tf.keras.layers.Dense(64,activation='relu',kernel_initializer='he_normal'),\n",
    "                                 tf.keras.layers.Dropout(0.5),\n",
    "                                 tf.keras.layers.Dense(2,activation='softmax',kernel_initializer='he_normal')])               \n",
    "    model.compile(optimizer=Adam(lr=0.001, epsilon=0.001,decay=1e-5, clipnorm=1.),loss = 'categorical_crossentropy',metrics=['categorical_accuracy',precision, recall, f1score])\n",
    "    \n",
    "    \n",
    "    return model\n",
    "def Eff_B6():\n",
    "    model = Sequential([efn.EfficientNetB6(input_shape=(136,136,3),weights='noisy-student',include_top=False),\n",
    "                                 tf.keras.layers.GlobalAveragePooling2D(),\n",
    "                                 tf.keras.layers.BatchNormalization(),\n",
    "                                 tf.keras.layers.Dense(128,activation='relu',kernel_initializer='he_normal'),\n",
    "                                 tf.keras.layers.Dense(64,activation='relu',kernel_initializer='he_normal'),\n",
    "                                 tf.keras.layers.Dropout(0.5),\n",
    "                                 tf.keras.layers.Dense(2,activation='softmax',kernel_initializer='he_normal')])               \n",
    "    model.compile(optimizer=Adam(lr=0.001, epsilon=0.001,decay=1e-5, clipnorm=1.),loss = 'categorical_crossentropy',metrics=['categorical_accuracy',precision, recall, f1score])\n",
    "    \n",
    "    \n",
    "    return model\n",
    "model_95=Eff_B7()\n",
    "model_93=Eff_B6()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../../data/.train/.task153/data/train'\n",
    "df = pd.read_csv(task_dir+'train.csv',names=['file','tile','label'],sep=',',skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '20201114'\n",
    "model = 'EFN_B7'\n",
    "version = 'V5_resize'\n",
    "batch_size = 30\n",
    "epochs = 50\n",
    "model_file ='Best_' + model +'_'+ version+'.h5'\n",
    "lrfn = build_lrfn()\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)\n",
    "EarlyStopping=tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",patience=5,verbose=True, mode=\"min\")\n",
    "modelsaver = tf.keras.callbacks.ModelCheckpoint(\n",
    "    model_file, \n",
    "    monitor='val_loss', \n",
    "    verbose=1, \n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "kf = KFold(n_splits=4)\n",
    "index=1\n",
    "for fold,(train,val) in enumerate(kf.split(df)):\n",
    "    print('Fold:', index)\n",
    "    train_df = df.iloc[train]\n",
    "    val_df = df.iloc[val]\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    val_df = val_df.reset_index(drop=True)\n",
    "    \n",
    "    train_N = train_df.shape[0]\n",
    "    train_x = np.empty((train_N, 136,136,3), dtype=np.uint8)\n",
    "    for i, path in enumerate(tqdm(train_df.file)):\n",
    "        image = (read_image(task_dir+path))\n",
    "        train_x[i,:,:,:] =  image\n",
    "    train_y = to_categorical(train_df['label'])\n",
    "    \n",
    "    val_N = val_df.shape[0]\n",
    "    val_x = np.empty((val_N, 136,136,3), dtype=np.uint8)\n",
    "    for i, path in enumerate(tqdm(val_df.file)):\n",
    "        image = (read_image(task_dir+path))\n",
    "        val_x[i,:,:,:] = image\n",
    "    val_y = to_categorical(val_df['label'])\n",
    "    \n",
    "    model = model_Eff.fit(train_x,\n",
    "                            train_y,\n",
    "                            epochs=epochs,\n",
    "                            validation_data = (val_x, val_y),\n",
    "                            verbose=2,\n",
    "                            steps_per_epoch=train.shape[0]//batch_size,\n",
    "                        callbacks=[lr_schedule,EarlyStopping,modelsaver])\n",
    "    \n",
    "    model_Eff.save('./'+date+'/'+model_file + '_'+str(index))\n",
    "    index+=1\n",
    "\n",
    "model_Eff.save('./'+'EFF'+version+ '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_PATH = '../../data/.train/.task153/data/test/'\n",
    "df_T = pd.read_csv(test_PATH+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_T\n",
    "test_N = df_T.shape[0]\n",
    "test_x = np.empty((test_N,136,136,3), dtype=np.uint8)\n",
    "for i,path in enumerate(tqdm(df_T.file_name)): \n",
    "    image = read_image(test_PATH + path)\n",
    "    test_x[i,:,:,:] =  image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "save_model = keras.models.load_model(\"./loadmodel.h5\",\n",
    "                                              custom_objects={\n",
    "                                                  'recall': recall,\n",
    "                                                  'precision': precision,\n",
    "                                                  'f1score' : f1score}\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
